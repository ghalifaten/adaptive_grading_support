{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94717e0d-6b18-47a1-a7ce-e7e8133ae25c",
   "metadata": {},
   "source": [
    "**Sign in to Hugging Face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1609053-0b08-4b59-98c9-38181324760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b94c76f2-8a96-4cab-8bda-a5c5f753e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2af8f8-0819-46c8-b1a1-a53ceff609b3",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a034f28-fd37-48ae-bbc3-3f411bd4e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data with 2 labels\n",
    "df = pd.read_csv(\"../data/clean/feedback_prize_2_labels.csv\")\n",
    "\n",
    "id2label = {0: \"CLAIM\", 1: \"PREMISE\"}\n",
    "label2id = {\"CLAIM\": 0, \"PREMISE\": 1}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46f686ff-9283-4633-89bb-15ef240bfbdd",
   "metadata": {},
   "source": [
    "#data with 7 labels\n",
    "df = pd.read_csv(\"../data/clean/feedback_prize_7_labels.csv\")\n",
    "\n",
    "id2label = {0: \"CLAIM\", \n",
    "            1: \"CONCLUDING STATEMENT\",\n",
    "            2: \"COUNTERCLAIM\",\n",
    "            3: \"EVIDENCE\",\n",
    "            4: \"LEAD\", \n",
    "            5: \"POSITION\",\n",
    "            6: \"REBUTTAL\"}\n",
    "\n",
    "label2id = {\"CLAIM\": 0,\n",
    "            \"CONCLUDING STATEMENT\": 1,\n",
    "            \"COUNTERCLAIM\": 2,\n",
    "            \"EVIDENCE\": 3,\n",
    "            \"LEAD\": 4, \n",
    "            \"POSITION\": 5,\n",
    "            \"REBUTTAL\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137c6abb-57ad-4b1c-b0fb-2227f39e6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\"\"\"\n",
    "split train and test set\n",
    "\"\"\"\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "tds = Dataset.from_pandas(train)\n",
    "vds = Dataset.from_pandas(test)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = tds\n",
    "ds['test'] = vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e24879-14fc-4ec3-95cc-b3fcab0d5dc0",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80fe87-5c8d-4f53-9580-6259a85f57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c2086-4b40-4eeb-a879-ad62845eaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERTâ€™s maximum input length\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540c0c7-ecbc-4161-b74a-1583dba453a0",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets map function. You can speed up map by setting batched=True to process multiple elements of the dataset at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6b0a5-d6ec-4728-93b6-ef0335077203",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2090b-ea64-4638-8c96-50f0839d1ad9",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorWithPadding. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximium length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ad221-ea83-42cb-9543-f43ad339e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26371bc5-d83c-4b06-9ec1-3e6cade75356",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ca6a2-f664-4e09-bd96-98897c60e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8665ed-c1d7-41a5-93b6-cac924e2ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    a function that passes predictions and labels to compute to calculate the accuracy\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca002a-bc78-4a21-84ea-95c5dff81daf",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6c180-a49c-4ed3-92af-fcb3311e6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c32dbe-b447-4a63-873d-ac7d7a1acf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1837d-a5b5-4af3-b7b1-bff90471f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_classification_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e01a9-b81e-4a60-bf42-57422ded8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ed_venv",
   "language": "python",
   "name": "ml4ed_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
